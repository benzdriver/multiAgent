import json
import asyncio
from pathlib import Path
from llm.llm_executor import run_prompt
from llm.chat_openai import chat
import tiktoken
import os
import re
from dependency_manager import DependencyManager, initialize_dependency_graph
from rollback_manager import RollbackManager, initialize_rollback_manager
from prompt_templates import get_fixer_prompt

BASE_PATH = Path("data/output/modules")
INDEX_PATH = Path("data/output/summary_index.json")
VALIDATOR_JSON_PATH = Path("data/validator_report.json")
FIX_LOG_PATH = Path("data/fix_log.md")

tokenizer = tiktoken.encoding_for_model("gpt-4o")

# å…¨å±€å¯¹è±¡
dependency_manager = None
rollback_manager = None

def get_fixer_prompt(module_name, issues, original_summary, related_modules=None):
    """ä»prompt_templatesåº“ä¸­è·å–ä¿®å¤prompt"""
    from prompt_templates import get_fixer_prompt as get_template_prompt
    return get_template_prompt(module_name, issues, original_summary, related_modules)

def load_summary(module_name):
    summary_path = BASE_PATH / module_name / "full_summary.json"
    if not summary_path.exists():
        return {
            "module_name": module_name,
            "responsibilities": [],
            "key_apis": [],
            "data_inputs": [],
            "data_outputs": [],
            "depends_on": [],
            "target_path": ""
        }
    return json.loads(summary_path.read_text())

def save_summary(module_name, summary):
    mod_dir = BASE_PATH / module_name
    mod_dir.mkdir(parents=True, exist_ok=True)
    path = mod_dir / "full_summary.json"
    path.write_text(json.dumps(summary, indent=2))

def update_index(summary_index, summary):
    summary_index[summary["module_name"]] = {
        "target_path": summary["target_path"],
        "depends_on": summary.get("depends_on", [])
    }

def get_issues_per_module(validator_report_path):
    """ä»éªŒè¯å™¨æŠ¥å‘Šä¸­æå–æ¯ä¸ªæ¨¡å—çš„é—®é¢˜
    
    Args:
        validator_report_path: éªŒè¯å™¨æŠ¥å‘Šæ–‡ä»¶çš„è·¯å¾„
        
    Returns:
        dict: ä»¥æ¨¡å—åä¸ºé”®ï¼Œé—®é¢˜åˆ—è¡¨ä¸ºå€¼çš„å­—å…¸
    """
    # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
    if not validator_report_path.exists():
        print(f"âŒ éªŒè¯å™¨æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {validator_report_path}")
        return {}
    
    try:
        # è¯»å–å¹¶è§£æJSONæ–‡ä»¶
        validator_data = json.loads(validator_report_path.read_text())
        
        structure_scan = validator_data.get("structure_scan", {})
        ai_review = validator_data.get("ai_review", {})
        boundary_analysis = validator_data.get("boundary_analysis", {})
        
        # æ”¶é›†ç»“æ„é—®é¢˜
        issue_map = {}
        for module, issues in structure_scan.items():
            if issues:  # åªæ·»åŠ æœ‰é—®é¢˜çš„æ¨¡å—
                issue_map[module] = ["Structure issue: " + issue for issue in issues]
        
        # æ·»åŠ è¾¹ç•Œåˆ†æé—®é¢˜
        merge_suggestions = boundary_analysis.get("merge_suggestions", [])
        for suggestion in merge_suggestions:
            modules = suggestion.get("modules", [])
            reason = suggestion.get("reason", "")
            for module in modules:
                if module in issue_map:
                    issue_map[module].append(f"Boundary issue: Consider merging with {', '.join([m for m in modules if m != module])} - {reason}")
                else:
                    issue_map[module] = [f"Boundary issue: Consider merging with {', '.join([m for m in modules if m != module])} - {reason}"]
        
        split_suggestions = boundary_analysis.get("split_suggestions", [])
        for suggestion in split_suggestions:
            module = suggestion.get("module", "")
            reason = suggestion.get("reason", "")
            if module:
                if module in issue_map:
                    issue_map[module].append(f"Boundary issue: Consider splitting module - {reason}")
                else:
                    issue_map[module] = [f"Boundary issue: Consider splitting module - {reason}"]
                
        # æ”¶é›†AIå»ºè®®ä¸­çš„é—®é¢˜
        overlaps = ai_review.get("overlapping_responsibilities", [])
        for overlap in overlaps:
            modules = overlap.split(" vs ")
            for module in modules:
                module = module.strip()
                if module in issue_map:
                    issue_map[module].append(f"Suggestion: {overlap}")
                else:
                    issue_map[module] = [f"Suggestion: {overlap}"]
                    
        # æ·»åŠ AIå»ºè®®ä¸­çš„å…·ä½“é’ˆå¯¹æŸæ¨¡å—çš„å»ºè®®
        suggestions = ai_review.get("suggestions", [])
        for suggestion in suggestions:
            # å°è¯•ä»å»ºè®®ä¸­æå–æ¨¡å—å
            for module in issue_map.keys():
                if module in suggestion:
                    issue_map[module].append(f"Suggestion: {suggestion}")
                    
        return issue_map
    except Exception as e:
        print(f"âŒ è§£æéªŒè¯å™¨æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        return {}

def parse_json_response(text: str) -> dict:
    """è§£æLLMè¿”å›çš„JSONå“åº”ï¼Œå¤„ç†å„ç§å¸¸è§çš„æ ¼å¼é—®é¢˜
    
    å¢å¼ºç‰ˆæœ¬: å¤„ç†æ›´å¤šè¾¹ç¼˜æƒ…å†µï¼Œç¡®ä¿æœ‰æ•ˆJSON
    """
    if not text:
        return {}
    
    # è®°å½•åŸå§‹æ–‡æœ¬ä»¥ä¾¿è°ƒè¯•
    original_text = text
    
    # 1. æ¸…ç†æ–‡æœ¬
    text = text.strip()
    
    # 2. ç§»é™¤markdownä»£ç å—æ ‡è®°
    code_block_pattern = r'```(?:json|javascript|js|JSON)?(.+?)```'
    matches = re.findall(code_block_pattern, text, re.DOTALL)
    if matches:
        # ä½¿ç”¨æ‰¾åˆ°çš„ç¬¬ä¸€ä¸ªä»£ç å—å†…å®¹
        text = matches[0].strip()
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»£ç å—ï¼Œä¹Ÿå¤„ç†å¯èƒ½çš„å•è¡Œä»£ç æ ‡è®°
        if text.startswith('```'):
            text = text[text.find('\n')+1:]
        if text.endswith('```'):
            text = text[:text.rfind('```')]
        text = text.strip()
    
    # 3. å°è¯•ç›´æ¥è§£æJSON
    try:
        parsed = json.loads(text)
        return parsed
    except json.JSONDecodeError:
        print(f"âš ï¸ åˆæ¬¡JSONè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤æ ¼å¼é—®é¢˜...")
    
    # 4. æŸ¥æ‰¾å¹¶æå–æœ€å¤–å±‚çš„JSONå¯¹è±¡
    try:
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹
        start_idx = text.find('{')
        end_idx = text.rfind('}')
        if start_idx != -1 and end_idx != -1 and start_idx < end_idx:
            extracted_text = text[start_idx:end_idx+1]
            try:
                return json.loads(extracted_text)
            except:
                print(f"âš ï¸ æå–JSONå¯¹è±¡åä»ç„¶æ— æ³•è§£æ")
    except Exception as e:
        print(f"âš ï¸ æå–JSONå¤±è´¥: {e}")
    
    # 5. å°è¯•ä¿®å¤å¸¸è§çš„JSONè¯­æ³•é—®é¢˜
    try:
        # 5.1 å°†å•å¼•å·æ›¿æ¢ä¸ºåŒå¼•å·ï¼ˆJavaScripté£æ ¼ -> JSONï¼‰
        fixed_text = re.sub(r"'([^']*)':", r'"\1":', text)
        # 5.2 ç»™æ²¡æœ‰å¼•å·çš„é”®æ·»åŠ å¼•å·
        fixed_text = re.sub(r'([{,])\s*([a-zA-Z0-9_]+):', r'\1"\2":', fixed_text)
        # 5.3 åˆ é™¤ç»“å°¾çš„é€—å·
        fixed_text = re.sub(r',\s*}', '}', fixed_text)
        fixed_text = re.sub(r',\s*]', ']', fixed_text)
        
        try:
            parsed = json.loads(fixed_text)
            print("âœ… æˆåŠŸé€šè¿‡ä¿®å¤JSONè¯­æ³•è§£æ")
            return parsed
        except:
            print("âš ï¸ ä¿®å¤JSONè¯­æ³•åä»ç„¶æ— æ³•è§£æ")
    except Exception as e:
        print(f"âš ï¸ ä¿®å¤JSONè¯­æ³•æ—¶å‡ºé”™: {e}")
    
    # 6. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°è¯•æ„å»ºJSONå¯¹è±¡
    try:
        # å°è¯•æå–æ‰€æœ‰é”®å€¼å¯¹
        pairs = re.findall(r'"([^"]+)"\s*:\s*("([^"\\]*(\\.[^"\\]*)*)"|\[.*?\]|{.*?}|true|false|null|-?\d+(\.\d+)?)', text)
        if pairs:
            json_obj = {}
            for pair in pairs:
                key = pair[0]
                value = pair[1]
                
                # å¤„ç†å­—ç¬¦ä¸²å€¼
                if value.startswith('"') and value.endswith('"'):
                    json_obj[key] = value[1:-1]
                # å¤„ç†æ•°ç»„å€¼
                elif value.startswith('[') and value.endswith(']'):
                    try:
                        json_obj[key] = json.loads(value)
                    except:
                        json_obj[key] = []
                # å¤„ç†å¯¹è±¡å€¼
                elif value.startswith('{') and value.endswith('}'):
                    try:
                        json_obj[key] = json.loads(value)
                    except:
                        json_obj[key] = {}
                # å¤„ç†å¸ƒå°”å€¼å’Œnull
                elif value in ['true', 'false', 'null']:
                    json_obj[key] = json.loads(value)
                # å¤„ç†æ•°å­—
                else:
                    try:
                        json_obj[key] = float(value) if '.' in value else int(value)
                    except:
                        json_obj[key] = value
            
            if json_obj:
                print("âœ… æˆåŠŸé€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æ„å»ºJSONå¯¹è±¡")
                return json_obj
    except Exception as e:
        print(f"âš ï¸ ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ„å»ºJSONæ—¶å‡ºé”™: {e}")
    
    # 7. æœ€åå°è¯•ï¼šå¦‚æœå®Œå…¨æ— æ³•è§£æï¼Œè¿”å›éƒ¨åˆ†ç»“æ„åŒ–æ•°æ®
    print(f"âŒ æ‰€æœ‰è§£ææ–¹æ³•å‡å¤±è´¥ï¼Œè¿”å›ç©ºå¯¹è±¡")
    print(f"åŸå§‹æ–‡æœ¬çš„å‰200ä¸ªå­—ç¬¦: {original_text[:200]}")
    return {}

def get_related_modules_context(module_name, all_modules):
    """è·å–ä¸æŒ‡å®šæ¨¡å—ç›¸å…³çš„æ¨¡å—ä¿¡æ¯
    
    Args:
        module_name: è¦è·å–ç›¸å…³æ¨¡å—çš„æ¨¡å—å
        all_modules: æ‰€æœ‰æ¨¡å—çš„æ‘˜è¦
        
    Returns:
        list: ç›¸å…³æ¨¡å—çš„ç®€åŒ–æ‘˜è¦åˆ—è¡¨
    """
    global dependency_manager
    
    related = []
    
    # å¦‚æœä¾èµ–å›¾å­˜åœ¨ï¼Œä½¿ç”¨ä¾èµ–å›¾è·å–ç›¸å…³æ¨¡å—
    if dependency_manager and module_name in dependency_manager.graph:
        # è·å–è¯¥æ¨¡å—ä¾èµ–çš„æ¨¡å—
        depends_on = dependency_manager.graph[module_name].get("depends_on", [])
        for dep in depends_on:
            for summary in all_modules:
                if summary.get("module_name") == dep:
                    # ç®€åŒ–æ‘˜è¦ï¼Œåªä¿ç•™é‡è¦ä¿¡æ¯
                    related.append({
                        "module_name": dep,
                        "relationship": "depended_on",
                        "responsibilities": summary.get("responsibilities", [])[:3],
                        "key_apis": summary.get("key_apis", [])[:3]
                    })
                    break
        
        # è·å–ä¾èµ–è¯¥æ¨¡å—çš„æ¨¡å—
        depended_by = dependency_manager.graph[module_name].get("depended_by", [])
        for dep in depended_by:
            for summary in all_modules:
                if summary.get("module_name") == dep:
                    # ç®€åŒ–æ‘˜è¦ï¼Œåªä¿ç•™é‡è¦ä¿¡æ¯
                    related.append({
                        "module_name": dep,
                        "relationship": "depends_on_this",
                        "responsibilities": summary.get("responsibilities", [])[:3],
                        "key_apis": summary.get("key_apis", [])[:3]
                    })
                    break
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„ç›¸å…³æ¨¡å—ï¼Œæ·»åŠ å‘½åç›¸ä¼¼çš„æ¨¡å—
    if len(related) < 3:
        # æå–åŸºæœ¬åç§°ï¼ˆå»æ‰Controllerã€Serviceã€Repositoryç­‰åç¼€ï¼‰
        base_name = re.sub(r'(Controller|Service|Repository|Page|Model)$', '', module_name)
        for summary in all_modules:
            other_name = summary.get("module_name", "")
            if other_name and other_name != module_name:
                other_base = re.sub(r'(Controller|Service|Repository|Page|Model)$', '', other_name)
                # å¦‚æœåŸºæœ¬åç§°ç›¸åŒï¼Œè®¤ä¸ºæ˜¯ç›¸å…³æ¨¡å—
                if other_base == base_name:
                    related.append({
                        "module_name": other_name,
                        "relationship": "similar_name",
                        "responsibilities": summary.get("responsibilities", [])[:3],
                        "key_apis": summary.get("key_apis", [])[:3]
                    })
    
    return related[:5]  # æœ€å¤šè¿”å›5ä¸ªç›¸å…³æ¨¡å—

async def run_incremental_validation(fixed_modules=None):
    """è¿è¡Œå¢é‡éªŒè¯ï¼ŒåªéªŒè¯ä¿®å¤è¿‡çš„æ¨¡å—"""
    # ä¿å­˜å½“å‰éªŒè¯æŠ¥å‘Šçš„é—®é¢˜è®¡æ•°
    if VALIDATOR_JSON_PATH.exists():
        try:
            old_report = json.loads(VALIDATOR_JSON_PATH.read_text())
            old_issues = count_total_issues(old_report)
        except Exception as e:
            print(f"âš ï¸ è¯»å–åŸå§‹éªŒè¯æŠ¥å‘Šå¤±è´¥: {e}")
            old_issues = 0
    else:
        old_issues = 0
    
    # å¯¼å…¥validatoræ¨¡å—
    try:
        from validator.validator import run_validator
        
        # è¿è¡Œå¢é‡éªŒè¯ï¼ŒåªéªŒè¯æŒ‡å®šçš„æ¨¡å—
        print(f"ğŸ” è¿è¡Œå¢é‡éªŒè¯ï¼Œå¯¹è±¡: {fixed_modules if fixed_modules else 'æ‰€æœ‰æ¨¡å—'}")
        validation_result = run_validator(modules_to_check=fixed_modules)
        
        # è·å–æ–°çš„é—®é¢˜æ•°é‡
        new_issues = validation_result["total_issues"]
        
        # è®¡ç®—å·®å¼‚
        diff = old_issues - new_issues
        
        return {
            "improved": diff >= 0,  # å…è®¸ç­‰äºï¼Œé¿å…å›æ»šæ— å˜åŒ–çš„ä¿®å¤
            "old_issues": old_issues,
            "new_issues": new_issues,
            "diff": diff
        }
    except Exception as e:
        print(f"âŒ è¿è¡ŒéªŒè¯å™¨å¤±è´¥: {e}")
        
        # å›é€€åˆ°è¿è¡Œå‘½ä»¤è¡ŒéªŒè¯
        import subprocess
        try:
            print("âš ï¸ å°è¯•ä½¿ç”¨å‘½ä»¤è¡Œè¿è¡Œå®Œæ•´éªŒè¯...")
            subprocess.run(["python", "run_validator.py"], check=True)
            
            # è¯»å–æ–°çš„éªŒè¯æŠ¥å‘Š
            if VALIDATOR_JSON_PATH.exists():
                new_report = json.loads(VALIDATOR_JSON_PATH.read_text())
                new_issues = count_total_issues(new_report)
            else:
                new_issues = 0
            
            # è®¡ç®—å·®å¼‚
            diff = old_issues - new_issues
            
            return {
                "improved": diff >= 0,
                "old_issues": old_issues,
                "new_issues": new_issues,
                "diff": diff
            }
        except Exception as subproc_err:
            print(f"âŒ å‘½ä»¤è¡ŒéªŒè¯ä¹Ÿå¤±è´¥: {subproc_err}")
            return {
                "improved": True,  # å‡è®¾æ”¹è¿›ä»¥é¿å…ä¸å¿…è¦çš„å›æ»š
                "old_issues": old_issues,
                "new_issues": old_issues,
                "diff": 0
            }

def count_total_issues(report):
    """è®¡ç®—æŠ¥å‘Šä¸­çš„æ€»é—®é¢˜æ•°"""
    count = 0
    
    # 1. ç»“æ„é—®é¢˜
    structure_scan = report.get("structure_scan", {})
    for module, issues in structure_scan.items():
        count += len(issues)
    
    # 2. AIå®¡æŸ¥é—®é¢˜
    ai_review = report.get("ai_review", {})
    
    # 2.1 é‡å è´£ä»»
    overlaps = ai_review.get("overlapping_responsibilities", [])
    count += len(overlaps)
    
    # 2.2 æœªå®šä¹‰ä¾èµ–
    undefined = ai_review.get("undefined_dependencies", [])
    count += len(undefined)
    
    # 2.3 ç¼ºå¤±æˆ–å†—ä½™æ¨¡å—
    missing_redundant = ai_review.get("missing_or_redundant_modules", {})
    count += len(missing_redundant.get("missing", []))
    count += len(missing_redundant.get("redundant", []))
    
    return count

def ensure_required_fields(module_data: dict, original_data: dict = None) -> dict:
    """ç¡®ä¿æ¨¡å—æ•°æ®åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µï¼Œå³ä½¿æ˜¯ç©ºå€¼"""
    required_fields = {
        "module_name": "",
        "responsibilities": [],
        "key_apis": [],
        "data_inputs": [],
        "data_outputs": [],
        "depends_on": [],
        "target_path": ""
    }
    
    result = {}
    
    # é¦–å…ˆä»åŸå§‹æ•°æ®å¡«å……
    if original_data:
        for field, default_value in required_fields.items():
            if field in original_data and original_data[field]:
                result[field] = original_data[field]
            else:
                result[field] = default_value
    
    # ç„¶åç”¨æ–°æ•°æ®è¦†ç›–
    for field, default_value in required_fields.items():
        if field in module_data and module_data[field] is not None:
            result[field] = module_data[field]
        elif field not in result:
            result[field] = default_value
    
    # ç¡®ä¿æ•°ç»„å­—æ®µä¸ºæ•°ç»„ç±»å‹
    for field in ["responsibilities", "key_apis", "data_inputs", "data_outputs", "depends_on"]:
        if not isinstance(result[field], list):
            # å°è¯•å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•°ç»„
            if isinstance(result[field], str):
                if result[field].startswith('[') and result[field].endswith(']'):
                    try:
                        result[field] = json.loads(result[field])
                    except:
                        result[field] = []
                else:
                    # å•ä¸ªé¡¹ç›®ä½œä¸ºæ•°ç»„çš„ä¸€ä¸ªå…ƒç´ 
                    result[field] = [result[field]] if result[field] else []
            else:
                result[field] = []
    
    # ç¡®ä¿module_nameæ˜¯å­—ç¬¦ä¸²
    if not isinstance(result["module_name"], str):
        result["module_name"] = str(result["module_name"]) if result["module_name"] else ""
    
    # ç¡®ä¿target_pathæ˜¯å­—ç¬¦ä¸²
    if not isinstance(result["target_path"], str):
        result["target_path"] = str(result["target_path"]) if result["target_path"] else ""
    
    return result

async def fix_modules():
    """ä¿®å¤éªŒè¯å™¨æŠ¥å‘Šä¸­æœ‰é—®é¢˜çš„æ¨¡å—"""
    global dependency_manager, rollback_manager
    
    # åˆå§‹åŒ–ä¾èµ–å›¾å’Œå›æ»šç®¡ç†å™¨
    if dependency_manager is None:
        dependency_manager = initialize_dependency_graph()
    
    if rollback_manager is None:
        rollback_manager = initialize_rollback_manager()
    
    # åœ¨ä¿®å¤å‰åˆ›å»ºæ£€æŸ¥ç‚¹
    rollback_manager.create_checkpoint("before_fix")
    
    # ä»éªŒè¯å™¨æŠ¥å‘Šä¸­è·å–æ¯ä¸ªæ¨¡å—çš„é—®é¢˜æ¸…å•
    issue_map = get_issues_per_module(VALIDATOR_JSON_PATH)
    
    # åŠ è½½æ‰€æœ‰æ¨¡å—çš„æ‘˜è¦
    all_modules = []
    for path in BASE_PATH.glob("*/full_summary.json"):
        try:
            all_modules.append(json.loads(path.read_text()))
        except:
            print(f"âš ï¸ Failed to parse {path}")
    
    # ä»summary_indexåŠ è½½ç´¢å¼•
    summary_index = json.loads(INDEX_PATH.read_text()) if INDEX_PATH.exists() else {}

    fix_log = []
    total = len(issue_map)
    print(f"ğŸ”§ Fixing {total} modules with issues...\n")

    # è·å–æ¨¡å—çš„æ‹“æ‰‘æ’åº
    ordered_modules = dependency_manager.get_topological_order()
    if ordered_modules is None:
        print("âš ï¸ æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼Œä½¿ç”¨é»˜è®¤æ’åº")
        modules = list(issue_map.items())
    else:
        # æŒ‰æ‹“æ‰‘æ’åºå¯¹æ¨¡å—è¿›è¡Œæ’åº
        modules = []
        # é¦–å…ˆåŒ…å«åœ¨æ‹“æ‰‘æ’åºä¸­ä¸”æœ‰é—®é¢˜çš„æ¨¡å—
        for mod_name in ordered_modules:
            if mod_name in issue_map:
                modules.append((mod_name, issue_map[mod_name]))
        # ç„¶ååŒ…å«æœªåœ¨æ‹“æ‰‘æ’åºä¸­çš„æ¨¡å—
        for mod_name, issues in issue_map.items():
            if mod_name not in ordered_modules:
                modules.append((mod_name, issues))

    # å°†æ¨¡å—åˆ†æˆå¤šä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æœ€å¤š3ä¸ªæ¨¡å—
    batch_size = 3
    batches = [modules[i:i + batch_size] for i in range(0, len(modules), batch_size)]
    
    for batch_idx, batch in enumerate(batches):
        print(f"\nğŸ“¦ Processing batch {batch_idx+1}/{len(batches)} ({len(batch)} modules)")
        
        # åœ¨æ¯ä¸ªæ‰¹æ¬¡å‰åˆ›å»ºæ£€æŸ¥ç‚¹
        batch_checkpoint = rollback_manager.create_checkpoint(f"batch_{batch_idx+1}")
        
        fixed_in_batch = []
        batch_had_failures = False
        
        for i, (mod_name, issue_list) in enumerate(batch):
            print(f"[{batch_idx*batch_size + i + 1}/{total}] Fixing {mod_name}...")
            original = load_summary(mod_name)
            
            # åˆ›å»ºæ¨¡å—çº§æ£€æŸ¥ç‚¹
            mod_checkpoint = rollback_manager.create_checkpoint(f"module_{mod_name}")
            
            # é™åˆ¶é—®é¢˜æ•°é‡ï¼Œä¼˜å…ˆé€‰æ‹©è§£æå’Œæœªå®šä¹‰ä¾èµ–é—®é¢˜
            prioritized_issues = []
            parsing_issues = [issue for issue in issue_list if "failed to parse" in issue or "unhashable type" in issue]
            dependency_issues = [issue for issue in issue_list if "undefined dependency" in issue]
            boundary_issues = [issue for issue in issue_list if "Boundary issue" in issue]
            other_issues = [issue for issue in issue_list if 
                           "failed to parse" not in issue and 
                           "unhashable type" not in issue and 
                           "undefined dependency" not in issue and
                           "Boundary issue" not in issue]
            
            # ä¼˜å…ˆå¤„ç†ä¸åŒç±»å‹çš„é—®é¢˜
            prioritized_issues.extend(parsing_issues)
            prioritized_issues.extend(dependency_issues[:max(0, 3-len(prioritized_issues))])
            prioritized_issues.extend(boundary_issues[:max(0, 3-len(prioritized_issues))])
            prioritized_issues.extend(other_issues[:max(0, 3-len(prioritized_issues))])
            
            limited_issues = prioritized_issues[:3]
            if len(issue_list) > 3:
                print(f"âš ï¸ Limiting to {len(limited_issues)} prioritized issues out of {len(issue_list)} total issues")
            
            # è·å–ç›¸å…³æ¨¡å—çš„ä¸Šä¸‹æ–‡
            related_modules = get_related_modules_context(mod_name, all_modules)
            
            # æ›´ç´§å‡‘çš„æç¤ºï¼Œå‡å°‘tokenæ•°é‡
            compact_summary = {
                "module_name": original.get("module_name", ""),
                "responsibilities": original.get("responsibilities", [])[:3],
                "key_apis": original.get("key_apis", [])[:3],
                "depends_on": original.get("depends_on", []),
                "target_path": original.get("target_path", "")
            }
            
            # æ„å»ºæç¤º
            fixer_prompt = get_fixer_prompt(mod_name, "\n".join(limited_issues), compact_summary, related_modules)
            user_prompt = f"Fix issues for module: {mod_name}"
            
            # ä¸æ–­å¢åŠ é‡è¯•æ¬¡æ•°å’Œç­‰å¾…æ—¶é—´ç­–ç•¥
            max_retries = 5  # å¢åŠ é‡è¯•æ¬¡æ•°
            success = False
            result = None
            fixed = None
            
            for attempt in range(max_retries):
                try:
                    # å‘LLMå‘é€æç¤º
                    result = await run_prompt(
                        chat=chat,
                        system_message=fixer_prompt,
                        user_message=user_prompt,
                        model="gpt-4o",
                        tokenizer=tokenizer,
                        parse_response=parse_json_response
                    )
                    
                    # ç¡®ä¿ç»“æœæ˜¯å­—å…¸è€Œä¸æ˜¯å­—ç¬¦ä¸²
                    if isinstance(result, str):
                        print(f"âš ï¸ Result is a string, attempting to parse...")
                        fixed = parse_json_response(result)
                    else:
                        fixed = result
                    
                    # æ£€æŸ¥ç»“æœæ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µ
                    required_fields = ["module_name", "responsibilities", "key_apis", "data_inputs", "data_outputs", "depends_on", "target_path"]
                    missing_fields = [field for field in required_fields if field not in fixed]
                    
                    if missing_fields:
                        print(f"âš ï¸ å°è¯• {attempt+1}: ç»“æœç¼ºå°‘å¿…éœ€å­—æ®µ: {missing_fields}")
                        # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
                        if attempt < max_retries - 1:
                            wait_time = min(10 * (attempt + 1), 30)  # é€æ¸å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œæœ€å¤š30ç§’
                            print(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                            await asyncio.sleep(wait_time)
                            continue
                    else:
                        # ç»“æœçœ‹èµ·æ¥æœ‰æ•ˆ
                        success = True
                        break
                        
                except Exception as e:
                    print(f"âŒ å°è¯• {attempt+1} å¤±è´¥: {str(e)}")
                    
                    # ä»…åœ¨éæœ€åä¸€æ¬¡å°è¯•æ—¶é‡è¯•
                    if attempt < max_retries - 1:
                        wait_time = min(10 * (attempt + 1), 30)  # é€æ¸å¢åŠ ç­‰å¾…æ—¶é—´ï¼Œæœ€å¤š30ç§’
                        print(f"â³ ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                        await asyncio.sleep(wait_time)
                    else:
                        print(f"âŒ æ‰€æœ‰é‡è¯•å°è¯•å‡å¤±è´¥ã€‚è·³è¿‡æ¨¡å— {mod_name}")
                        batch_had_failures = True
            
            # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæ¨¡å—
            if not success:
                print(f"âŒ æ— æ³•ä¿®å¤æ¨¡å— {mod_name}ï¼Œè·³è¿‡")
                batch_had_failures = True
                continue
            
            # å¤„ç†å¯èƒ½çš„åµŒå¥—å­—å…¸ï¼Œç¡®ä¿æ‰€æœ‰å€¼éƒ½æ˜¯ç®€å•ç±»å‹
            try:
                for key, value in list(fixed.items()):
                    if isinstance(value, dict):
                        print(f"âš ï¸ å°†'{key}'ä¸­çš„åµŒå¥—å­—å…¸è½¬æ¢ä¸ºå­—ç¬¦ä¸²")
                        fixed[key] = str(value)
            except Exception as e:
                print(f"âš ï¸ å¤„ç†åµŒå¥—å­—å…¸æ—¶å‡ºé”™: {e}")
            
            print(f"âœ“ è·å–æœ‰æ•ˆçš„JSONå“åº”")
            
            try:
                # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨ï¼Œä½¿ç”¨åŸå§‹æ•°æ®ä½œä¸ºå¤‡ç”¨
                fixed = ensure_required_fields(fixed, original)
                
                # æœ€ç»ˆæ£€æŸ¥æ‰€æœ‰å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨
                for field in ["module_name", "target_path", "responsibilities", "depends_on"]:
                    if field not in fixed or fixed[field] is None:
                        raise ValueError(f"ä»ç„¶ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                
                # æ£€æŸ¥ä¾èµ–æ˜¯å¦å¼•å…¥å¾ªç¯
                new_dependencies = fixed.get("depends_on", [])
                old_dependencies = original.get("depends_on", [])
                
                # å¦‚æœä¾èµ–æœ‰å˜åŒ–ï¼Œæ£€æŸ¥å¾ªç¯ä¾èµ–
                if set(new_dependencies) != set(old_dependencies):
                    # æ›´æ–°ä¾èµ–å›¾
                    dependency_check = dependency_manager.update_dependencies(mod_name, new_dependencies)
                    
                    # å¦‚æœå¼•å…¥äº†å¾ªç¯ä¾èµ–ï¼Œè­¦å‘Šå¹¶æä¾›è¯¦ç»†ä¿¡æ¯
                    if dependency_check["has_cycles"]:
                        cycles = dependency_check["cycles"]
                        print(f"âš ï¸ æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–: {cycles}")
                        
                        # å¦‚æœå¾ªç¯ä¾èµ–æ¶‰åŠå½“å‰æ¨¡å—ï¼Œå°è¯•ç§»é™¤å¯¼è‡´å¾ªç¯çš„ä¾èµ–
                        for cycle in cycles:
                            if mod_name in cycle:
                                for dep in cycle:
                                    if dep in new_dependencies and dep != mod_name:
                                        print(f"âš ï¸ ç§»é™¤å¯¼è‡´å¾ªç¯ä¾èµ–çš„ä¾èµ–: {dep}")
                                        new_dependencies.remove(dep)
                        
                        # æ›´æ–°ç§»é™¤å¾ªç¯ä¾èµ–åçš„ä¾èµ–åˆ—è¡¨
                        fixed["depends_on"] = new_dependencies
                        dependency_manager.update_dependencies(mod_name, new_dependencies)
                
                # ä¿å­˜ä¿®å¤åçš„æ¨¡å—
                save_summary(mod_name, fixed)
                update_index(summary_index, fixed)
                fix_log.append(f"### âœ… {mod_name} fixed:\n" + "\n".join(limited_issues) + "\n")
                fixed_in_batch.append(mod_name)
                print(f"âœ… æˆåŠŸä¿®å¤ {mod_name}")
            except Exception as e:
                print(f"âŒ ä¿å­˜/æ›´æ–° {mod_name} æ—¶å‡ºé”™: {e}")
                print(f"Fixed data: {fixed}")
                batch_had_failures = True
                continue
        
        # åœ¨æ¯ä¸ªæ‰¹æ¬¡åä¿å­˜ç´¢å¼•å’Œæ—¥å¿—ï¼Œä»¥å…ä¸­é€”å¤±è´¥ä¸¢å¤±æ•°æ®
        INDEX_PATH.write_text(json.dumps(summary_index, indent=2))
        FIX_LOG_PATH.write_text("\n".join(fix_log))
        print(f"ğŸ”„ æ‰¹æ¬¡ {batch_idx+1} å®Œæˆåä¿å­˜è¿›åº¦")
        
        # æ¯ä¸ªæ‰¹æ¬¡åè¿›è¡Œå¢é‡éªŒè¯
        if fixed_in_batch:
            print("ğŸ” è¿è¡Œå¢é‡éªŒè¯...")
            validation_result = await run_incremental_validation(fixed_in_batch)
            
            print(f"ğŸ“Š éªŒè¯ç»“æœ: {validation_result['old_issues']} -> {validation_result['new_issues']} é—®é¢˜ (å·®å¼‚: {validation_result['diff']})")
            
            # å¦‚æœé—®é¢˜å¢åŠ äº†ï¼Œå›æ»šåˆ°æ‰¹æ¬¡æ£€æŸ¥ç‚¹
            if validation_result['diff'] < -2:  # åªæœ‰é—®é¢˜æ˜æ˜¾å¢åŠ æ‰å›æ»šï¼Œå…è®¸å°æ³¢åŠ¨
                print(f"âš ï¸ æ£€æµ‹åˆ°é—®é¢˜æ˜¾è‘—å¢åŠ ï¼å›æ»šåˆ°æ‰¹æ¬¡æ£€æŸ¥ç‚¹: {batch_checkpoint}")
                rollback_manager.rollback_to_checkpoint(batch_checkpoint)
                
                # ä»ä¾èµ–å›¾ä¸­ç§»é™¤åˆšåˆšä¿®å¤çš„æ¨¡å—çš„ä¾èµ–æ›´æ–°
                for mod_name in fixed_in_batch:
                    # æ¢å¤åŸå§‹ä¾èµ–
                    original = load_summary(mod_name)
                    original_deps = original.get("depends_on", [])
                    dependency_manager.update_dependencies(mod_name, original_deps)
                
                # ä»ä¿®å¤æ—¥å¿—ä¸­ç§»é™¤å›æ»šçš„æ¨¡å—
                for mod_name in fixed_in_batch:
                    fix_log = [log for log in fix_log if not log.startswith(f"### âœ… {mod_name} fixed:")]
                
                # æ›´æ–°ä¿®å¤æ—¥å¿—
                FIX_LOG_PATH.write_text("\n".join(fix_log))
            elif batch_had_failures and validation_result['diff'] < 0:
                # å¦‚æœæ‰¹æ¬¡æœ‰å¤±è´¥ä¸”é—®é¢˜å¢åŠ ï¼Œä¹Ÿå›æ»š
                print(f"âš ï¸ æ‰¹æ¬¡æœ‰å¤±è´¥ä¸”é—®é¢˜å¢åŠ ã€‚å›æ»šåˆ°æ‰¹æ¬¡æ£€æŸ¥ç‚¹: {batch_checkpoint}")
                rollback_manager.rollback_to_checkpoint(batch_checkpoint)
                
                # ä»ä¾èµ–å›¾ä¸­ç§»é™¤åˆšåˆšä¿®å¤çš„æ¨¡å—çš„ä¾èµ–æ›´æ–°
                for mod_name in fixed_in_batch:
                    # æ¢å¤åŸå§‹ä¾èµ–
                    original = load_summary(mod_name)
                    original_deps = original.get("depends_on", [])
                    dependency_manager.update_dependencies(mod_name, original_deps)
                
                # ä»ä¿®å¤æ—¥å¿—ä¸­ç§»é™¤å›æ»šçš„æ¨¡å—
                for mod_name in fixed_in_batch:
                    fix_log = [log for log in fix_log if not log.startswith(f"### âœ… {mod_name} fixed:")]
                
                # æ›´æ–°ä¿®å¤æ—¥å¿—
                FIX_LOG_PATH.write_text("\n".join(fix_log))
            else:
                # å³ä½¿é—®é¢˜æ²¡æœ‰å‡å°‘ï¼Œä½†ä¹Ÿæ²¡æœ‰æ˜æ˜¾å¢åŠ ï¼Œä»ç„¶ä¿ç•™ä¿®å¤
                if validation_result['diff'] < 0:
                    print(f"â„¹ï¸ é—®é¢˜ç•¥æœ‰å¢åŠ  (diff: {validation_result['diff']})ï¼Œä½†åœ¨å®¹å¿èŒƒå›´å†…ï¼Œä¿ç•™ä¿®å¤")
                else:
                    print(f"âœ… ä¿®å¤æˆåŠŸï¼Œé—®é¢˜å‡å°‘: {validation_result['diff']}")
                    
                # æ¯ä¿®å¤æˆåŠŸ5ä¸ªæ¨¡å—åï¼Œä¿å­˜ä¸€ä¸ªæ–°çš„æ£€æŸ¥ç‚¹ç”¨äºå¯èƒ½çš„å›æ»š
                if len(fix_log) % 15 == 0:
                    rollback_manager.create_checkpoint(f"milestone_{len(fix_log)}_fixes")
        
        # æ‰¹æ¬¡ä¹‹é—´æš‚åœä¸€ä¸‹ï¼Œé¿å…è¿‡å¤šçš„APIè¯·æ±‚
        if batch_idx < len(batches) - 1:
            print(f"â³ æš‚åœ10ç§’é’Ÿåå¤„ç†ä¸‹ä¸€æ‰¹æ¬¡...")
            await asyncio.sleep(10)

    print(f"\nâœ… æ‰€æœ‰æ¨¡å—ä¿®å¤å®Œæˆã€‚æ—¥å¿—ä¿å­˜åˆ° {FIX_LOG_PATH}")
    
    # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
    rollback_manager.cleanup_old_checkpoints()
    
    # ç”Ÿæˆæœ€ç»ˆçš„ä¾èµ–å›¾å¯è§†åŒ–
    dependency_manager.visualize()

if __name__ == "__main__":
    # åˆå§‹åŒ–å…¨å±€å¯¹è±¡
    dependency_manager = initialize_dependency_graph()
    rollback_manager = initialize_rollback_manager()
    
    asyncio.run(fix_modules())