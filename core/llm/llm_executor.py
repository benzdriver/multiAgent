import re
import asyncio
from typing import Callable, Optional, Any, List
from core.llm.token_splitter import split_text_by_tokens
import tiktoken
import json

TERMINATION_PATTERNS = [
    r"\.{3,}\s*$",         # ... or ......
    r"(TERMINATE|END|CONTINUE|TO BE CONTINUED)\s*$"
]

def _seems_incomplete(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦çœ‹èµ·æ¥ä¸å®Œæ•´ï¼Œä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²æ“ä½œè€Œä¸æ˜¯æ­£åˆ™è¡¨è¾¾å¼"""
    print(f"ğŸ” æ£€æŸ¥æ–‡æœ¬å®Œæ•´æ€§ (æœ€å100å­—ç¬¦: {text[-100:] if len(text) > 100 else text})")
    if not text:
        print("âš ï¸ ç©ºæ–‡æœ¬")
        return False
        
    # åªæ£€æŸ¥æœ€å100ä¸ªå­—ç¬¦ï¼Œé¿å…å¤„ç†è¿‡é•¿çš„æ–‡æœ¬
    last_part = text[-100:].strip().upper()
    
    # ç®€å•çš„å­—ç¬¦ä¸²æ£€æŸ¥ï¼Œä¸ä½¿ç”¨æ­£åˆ™
    if last_part.endswith('...'):
        print("ğŸ“ æ£€æµ‹åˆ°çœç•¥å·ç»“å°¾")
        return True
    if last_part.endswith('CONTINUE'):
        print("ğŸ“ æ£€æµ‹åˆ°CONTINUEç»“å°¾")
        return True
    if last_part.endswith('TO BE CONTINUED'):
        print("ğŸ“ æ£€æµ‹åˆ°TO BE CONTINUEDç»“å°¾")
        return True
    if last_part.endswith('END'):
        print("ğŸ“ æ£€æµ‹åˆ°ENDç»“å°¾")
        return True
    if last_part.endswith('TERMINATE'):
        print("ğŸ“ æ£€æµ‹åˆ°TERMINATEç»“å°¾")
        return True
        
    print("âœ… æ–‡æœ¬å®Œæ•´")
    return False

async def mock_llm_call(prompt: str, return_json: bool = False) -> Any:
    """æä¾›æ¨¡æ‹Ÿçš„LLMå“åº”ï¼Œç”¨äºæµ‹è¯•æˆ–æ— æ³•è¿æ¥LLMçš„æƒ…å†µ
    
    Args:
        prompt: æç¤ºè¯
        return_json: æ˜¯å¦è¿”å›JSONæ ¼å¼
        
    Returns:
        æ¨¡æ‹Ÿçš„LLMå“åº”
    """
    print("âš ï¸ ä½¿ç”¨æ¨¡æ‹ŸLLMå“åº”")
    
    # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
    await asyncio.sleep(1)
    
    # æ£€æµ‹æç¤ºä¸­æ˜¯å¦è¦æ±‚JSONæ ¼å¼
    json_requested = return_json or "JSON" in prompt or "json" in prompt
    
    if json_requested:
        # è¿”å›ä¸€ä¸ªåŸºæœ¬çš„JSONç»“æ„
        response = {
            "status": "success",
            "message": "è¿™æ˜¯æ¨¡æ‹Ÿçš„LLM JSONå“åº”",
            "timestamp": str(asyncio.get_event_loop().time()),
            "data": {
                "analysis": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿåˆ†æç»“æœ",
                "details": ["é¡¹ç›®1", "é¡¹ç›®2", "é¡¹ç›®3"],
                "recommendation": "è¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿæ¨è"
            }
        }
        return response
    else:
        # è¿”å›æ–‡æœ¬å“åº”
        return """è¿™æ˜¯æ¨¡æ‹Ÿçš„LLMæ–‡æœ¬å“åº”ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œä¼šè¿”å›ä¸€ä¸ªçœŸå®çš„LLMç”Ÿæˆå†…å®¹ã€‚
        
è¯¥å“åº”ç”¨äºå¼€å‘å’Œæµ‹è¯•é˜¶æ®µï¼Œä¸åº”åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ã€‚è¯·ç¡®ä¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­è¿æ¥å®é™…çš„LLMæœåŠ¡ã€‚"""

async def _run_with_continuation(
    chat: Callable[..., asyncio.Future],
    task: Optional[str] = None,
    system_prompt: Optional[str] = None,
    user_prompt: Optional[str] = None,
    max_steps: int = 3,
    model: str = "gpt-4o"
) -> str:
    """è¿è¡Œå¸¦æœ‰è‡ªåŠ¨ç»§ç»­åŠŸèƒ½çš„èŠå¤©ï¼Œä½¿ç”¨éé€’å½’æ–¹å¼å¤„ç†å“åº”"""
    print(f"\nğŸ”„ å¼€å§‹è¿è¡Œcontinuation (max_steps={max_steps})")
    assert task or (system_prompt and user_prompt), "å¿…é¡»æä¾›taskæˆ–(system_promptå’Œuser_prompt)"

    # å‡†å¤‡åˆå§‹æç¤º
    if task:
        print(f"ğŸ“ ä½¿ç”¨taskæ¨¡å¼ (é•¿åº¦: {len(task)}å­—ç¬¦)")
        current_messages = [{"role": "user", "content": task}]
    else:
        print(f"ğŸ“ ä½¿ç”¨system+useræ¨¡å¼ (systemé•¿åº¦: {len(system_prompt or '')}å­—ç¬¦, useré•¿åº¦: {len(user_prompt or '')}å­—ç¬¦)")
        current_messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]

    # æ”¶é›†æ‰€æœ‰å“åº”
    all_responses = []
    
    # è¿­ä»£å¤„ç†ï¼Œæœ€å¤šmax_stepsæ¬¡
    for step in range(max_steps):
        try:
            print(f"\nğŸ”„ å¼€å§‹ç¬¬ {step+1} æ­¥å¤„ç†")
            print(f"ğŸ“¤ å‘é€æ¶ˆæ¯ (æ•°é‡: {len(current_messages)})")
            
            # å‘é€è¯·æ±‚
            result = await chat(messages=current_messages, model=model)
            
            # è·å–å“åº”æ–‡æœ¬
            response_text = result if isinstance(result, str) else result.messages[-1].content
            print(f"ğŸ“¥ æ”¶åˆ°å“åº” (é•¿åº¦: {len(response_text)}å­—ç¬¦)")
            all_responses.append(response_text.strip())
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­
            if not _seems_incomplete(response_text):
                print("âœ… å“åº”å®Œæ•´ï¼Œç»“æŸå¤„ç†")
                break
                
            print(f"ğŸ” æ£€æµ‹åˆ°ä¸å®Œæ•´çš„è¾“å‡º (æ­¥éª¤ {step+1})ï¼Œç»§ç»­...")
            
            # æ·»åŠ ç»§ç»­æç¤º
            current_messages.append({"role": "assistant", "content": response_text})
            current_messages.append({"role": "user", "content": "è¯·ç»§ç»­ä¸Šä¸€ä¸ªå›ç­”ã€‚"})
            
        except Exception as e:
            print(f"âš ï¸ æ­¥éª¤ {step+1} å‡ºé”™: {str(e)}")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            break

    # åˆå¹¶æ‰€æœ‰å“åº”
    final_response = "\n".join(all_responses).strip()
    print(f"âœ… continuationå®Œæˆ (æœ€ç»ˆé•¿åº¦: {len(final_response)}å­—ç¬¦)")
    return final_response

def _merge_sections(acc, current, depth=0):
    """éé€’å½’çš„åˆå¹¶é€»è¾‘ï¼Œç®€å•åœ°å°†ç»“æœæ”¶é›†åˆ°åˆ—è¡¨ä¸­
    
    Args:
        acc: ç´¯ç§¯çš„ç»“æœ
        current: å½“å‰éœ€è¦åˆå¹¶çš„ç»“æœ
        depth: é€’å½’æ·±åº¦ï¼ˆæ­¤å‚æ•°ä¿ç•™ä½†ä¸å†ä½¿ç”¨ï¼‰
        
    Returns:
        åˆå¹¶åçš„ç»“æœ
    """
    print("ğŸ”„ æ”¶é›†æ–°çš„æ•°æ®å—")
    
    # å¦‚æœæ²¡æœ‰ä¹‹å‰çš„ç»“æœï¼Œåˆ›å»ºæ–°åˆ—è¡¨
    if not hasattr(_merge_sections, 'all_results'):
        _merge_sections.all_results = []
    
    # æ·»åŠ æ–°çš„ç»“æœåˆ°åˆ—è¡¨
    if current:
        _merge_sections.all_results.append(current)
        print(f"ğŸ“¦ å·²æ”¶é›† {len(_merge_sections.all_results)} ä¸ªæ•°æ®å—")
    
    # å¦‚æœaccä¸ºç©ºï¼Œè¿”å›å½“å‰ç»“æœ
    if not acc:
        return current
    
    # å¦åˆ™è¿”å›acc
    return acc

async def run_prompt(
    chat: Callable = None,
    *,
    messages: Optional[List[dict]] = None,
    user_message: Optional[str] = None,
    system_message: Optional[str] = None,
    model: str = "gpt-4o",
    tokenizer=None,
    max_input_tokens: int = 16000,  # å¢å¤§æœ€å¤§è¾“å…¥tokenæ•°
    parse_response: Callable[[str], Any] = lambda x: x,
    merge_result: Callable[[Any, Any], Any] = lambda acc, x: x,
    get_system_prompt: Optional[Callable[[int, int], str]] = None,
    use_pipeline: bool = False,
    use_mock: bool = False,
    return_json: bool = False
) -> Any:
    """
    ç»Ÿä¸€çš„ LLM prompt è°ƒç”¨æ¥å£ã€‚

    å‚æ•°ä¼˜å…ˆçº§ï¼š
    1. messagesï¼šå®Œæ•´å†å²ï¼Œç›´æ¥ä¼ é€’ç»™ LLMã€‚
    2. system_message + user_messageï¼šç»„è£…æˆ system+user ä¸¤æ¡æ¶ˆæ¯ã€‚
    3. user_messageï¼šå•è½® user æ¶ˆæ¯ã€‚

    Args:
        chat: LLM èŠå¤©å‡½æ•°ã€‚
        messages: å®Œæ•´æ¶ˆæ¯å†å²ï¼ˆList[Dict]ï¼‰ã€‚
        user_message: å•è½® user æ¶ˆæ¯ã€‚
        system_message: å•è½® system æ¶ˆæ¯ã€‚
        model: æ¨¡å‹åç§°ã€‚
        tokenizer: åˆ†è¯å™¨ã€‚
        max_input_tokens: æœ€å¤§è¾“å…¥ token æ•°ã€‚
        parse_response: è§£æå“åº”çš„å‡½æ•°ã€‚
        merge_result: åˆå¹¶ç»“æœçš„å‡½æ•°ã€‚
        get_system_prompt: åˆ†å—æ—¶è‡ªå®šä¹‰ system promptã€‚
        use_pipeline: æ˜¯å¦æµæ°´çº¿å¤„ç†ã€‚
        use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿå“åº”ã€‚
        return_json: æ˜¯å¦è¿”å› JSONã€‚
    Returns:
        LLM å“åº”ç»“æœã€‚
    """
    print(f"\nğŸš€ å¼€å§‹è¿è¡Œprompt (æ¨¡å‹: {model})")
    print(f"ğŸ“Š é…ç½®: max_input_tokens={max_input_tokens}, use_mock={use_mock}")

    # ä¼˜å…ˆçº§ï¼šmessages > (system_message + user_message) > user_message
    if messages:
        input_messages = messages
    elif system_message and user_message:
        input_messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]
    elif user_message:
        input_messages = [
            {"role": "user", "content": user_message}
        ]
    else:
        raise ValueError("å¿…é¡»æä¾› messagesï¼Œæˆ– (system_message + user_message)ï¼Œæˆ– user_message å…¶ä¸­ä¹‹ä¸€")

    # å¦‚æœè¯·æ±‚ä½¿ç”¨æ¨¡æ‹Ÿå“åº”ï¼Œç›´æ¥è¿”å›
    if use_mock or (chat is None):
        print("ğŸ”„ ä½¿ç”¨æ¨¡æ‹ŸLLMå“åº”")
        combined_prompt = "\n".join([m.get("content", "") for m in input_messages])
        return await mock_llm_call(combined_prompt, return_json)

    # è‡ªåŠ¨åˆ›å»ºtokenizer
    if tokenizer is None:
        print(f"ğŸ“ è‡ªåŠ¨åˆ›å»ºtokenizer (æ¨¡å‹: {model})")
        tokenizer = tiktoken.encoding_for_model(model)
        print("âœ… tokenizeråˆ›å»ºæˆåŠŸ")

    # è®¡ç®— token æ•°
    input_text = "\n".join([m.get("content", "") for m in input_messages])
    try:
        tokens = tokenizer.encode(input_text)
        token_count = len(tokens)
        print(f"ğŸ“ è¾“å…¥æ–‡æœ¬ç»Ÿè®¡:")
        print(f"  - å­—ç¬¦æ•°: {len(input_text)}")
        print(f"  - Tokenæ•°: {token_count}")
        print(f"  - Token/å­—ç¬¦æ¯”: {token_count/len(input_text):.2f}")
    except Exception as e:
        print(f"âš ï¸ Tokenè®¡ç®—å‡ºé”™: {str(e)}")
        raise

    # å¦‚æœæ–‡æœ¬è¶³å¤ŸçŸ­ï¼Œç›´æ¥å¤„ç†
    if token_count <= max_input_tokens:
        print(f"ğŸ“ æ–‡æœ¬åœ¨å…è®¸èŒƒå›´å†…ï¼Œç›´æ¥å‘é€")
        try:
            result = await chat(messages=input_messages, model=model)
            parsed = parse_response(result if isinstance(result, str) else result)
            print("âœ… ç›´æ¥å¤„ç†å®Œæˆ")
            return parsed
        except Exception as e:
            print(f"âš ï¸ ç›´æ¥å¤„ç†æ—¶å‡ºé”™: {str(e)}")
            raise

    # åˆ†å—å¤„ç†é•¿æ–‡æœ¬
    print(f"\nğŸ“ æ–‡æœ¬è¿‡é•¿ï¼Œå¼€å§‹åˆ†å—å¤„ç†")
    try:
        print("ğŸ”„ è°ƒç”¨split_text_by_tokens...")
        chunks = split_text_by_tokens(input_text, tokenizer, max_tokens=max_input_tokens)
        print(f"ğŸ“¦ åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")
        for i, chunk in enumerate(chunks):
            chunk_tokens = len(tokenizer.encode(chunk))
            print(f"  å— {i+1}: {len(chunk)}å­—ç¬¦, {chunk_tokens} tokens")
    except Exception as e:
        print(f"âš ï¸ åˆ†å—è¿‡ç¨‹å‡ºé”™: {str(e)}")
        raise

    # é™åˆ¶å—æ•°é‡
    if len(chunks) > 10:
        print(f"âš ï¸ å—æ•°é‡è¿‡å¤š ({len(chunks)})ï¼Œé™åˆ¶ä¸ºå‰10ä¸ªå—")
        chunks = chunks[:10]

    # å¤„ç†æ¯ä¸ªå—
    final_result = None
    for i, chunk in enumerate(chunks):
        print(f"\nğŸ›°ï¸ å¼€å§‹å¤„ç†å— {i+1}/{len(chunks)}")
        print(f"ğŸ“Š å—å¤§å°: {len(chunk)}å­—ç¬¦, {len(tokenizer.encode(chunk))} tokens")
        # åˆ†å—æ—¶å¯è‡ªå®šä¹‰ system prompt
        if get_system_prompt:
            current_system_message = get_system_prompt(i + 1, len(chunks))
        else:
            current_system_message = system_message
        chunk_messages = (
            [{"role": "system", "content": current_system_message}] if current_system_message else []
        ) + [{"role": "user", "content": chunk}]
        try:
            response = await chat(messages=chunk_messages, model=model)
            parsed = parse_response(response if isinstance(response, str) else response)
            print(f"âœ… å— {i+1} å¤„ç†å®Œæˆ")
            if final_result is None:
                final_result = parsed
                print("ğŸ“ è®¾ç½®åˆå§‹ç»“æœ")
            else:
                print("ğŸ”„ åˆå¹¶ç»“æœ...")
                final_result = merge_result(final_result, parsed)
                print("âœ… ç»“æœåˆå¹¶å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ å¤„ç†å— {i+1} æ—¶å‡ºé”™:")
            print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            print(f"é”™è¯¯è¯¦æƒ…: {str(e)}")
            continue
    print("\nâœ… æ‰€æœ‰å—å¤„ç†å®Œæˆ")
    return final_result
